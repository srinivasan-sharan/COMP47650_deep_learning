{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer Feedforward Neural Network #\n",
    "\n",
    "Objectives: create and train a single layer feedforward neural network building on the approach taken in the previous logistic regression notebook. For visualisation purposes (without loss of generality) we will use 2D datasets.\n",
    "\n",
    "> **Instructions:** Ensure your Python environment is setup with the following additional packages: \n",
    ">\n",
    "> - [sklearn](http://scikit-learn.org/stable/) is simple and efficient tools for data mining and data analysis\n",
    "> - `t3utils.py` contains unit tests to check your code and some visualiation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import t3utils as t3\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Preparing your Dataset ##\n",
    "\n",
    "First, let's generate a dataset to work on. The following code will create a 2-class planar dataset with the same interface used in our previous tutorial. It uses a parametric equation to create a 6 petal spirograph split into two classes. The feature dimension is 2 as opposed to $N\\times N\\times 3$ in our previous image dataset. Therefore data can be conveniently mapped to coordinates in cartesian plane. Evaluate the following cells to generate both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \"\"\"\n",
    "    create dataset\n",
    "    \n",
    "    Returns:\n",
    "    (X_train, Y_train), (X_test, Y_test), classes -- training and test data\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2019) # for reproducibility\n",
    "    n = 256\n",
    "    X = np.zeros((2*n,2))\n",
    "    Y = np.zeros((2*n,1), dtype='uint8')\n",
    "    classes = ['red', 'blue']\n",
    "\n",
    "    for k in range(2):\n",
    "        i = range(k*n, (k+1)*n)\n",
    "        Y[i, 0] = k\n",
    "        # spirograph data points\n",
    "        theta = np.linspace(k*np.pi, 2*(k+1)*np.pi, n)\n",
    "        rho = np.sin(3*(theta+k*np.pi)) + np.random.randn(n) * 0.1\n",
    "        X[i] = np.c_[rho*np.sin(theta), rho*np.cos(theta)]\n",
    "    \n",
    "    \n",
    "    index = np.random.choice(2*n, size=int(0.05 * 2*n), replace=False)    \n",
    "    Y_s = Y[index, 0]\n",
    "    np.random.shuffle(Y_s)\n",
    "    Y[index, 0] = Y_s\n",
    "    \n",
    "    X_train = np.concatenate((X[0:int(n/2),:], X[n:3*int(n/2),:]))\n",
    "    X_test = np.concatenate((X[int(n/2):n,:], X[3*int(n/2):2*n,:]))\n",
    "    Y_train = np.concatenate((Y[0:int(n/2),:], Y[n:3*int(n/2),:]))\n",
    "    Y_test = np.concatenate((Y[int(n/2):n,:], Y[3*int(n/2):2*n,:]))\n",
    "                             \n",
    "    return (X_train, Y_train), (X_test, Y_test), classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='darkgreen'>**Exercise 1:**</font> Complete the following code to explore dataset dimensions and visualise the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test), classes = load_dataset()\n",
    "\n",
    "### INPUT YOUR CODE HERE ### (3 lines)\n",
    "n_train = None\n",
    "n_test = None\n",
    "feature_count = None\n",
    "### END OF YOUR CODE SEGMENT ###  \n",
    "\n",
    "print(\"training set: {} images\".format(n_train))\n",
    "print(\"test set: {} images\".format(n_test))\n",
    "print(\"features per sample: {} scalars\".format(feature_count))\n",
    "plt.scatter(X_test[:,0], X_test[:,1], c=Y_test.reshape(-1), cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> training set: 256 images\n",
    "> test set: 256 images\n",
    "> features per sample: 2 scalars\n",
    "> ```\n",
    ">\n",
    "> `X_train` and `X_test` are numpy matrices with two features $(x_1, x_2)$ on each row. `Y_train` and `Y_test` contain our labels, 0 for the `red` class, 1 for the `blue` class, organised as a column vector.\n",
    ">\n",
    "> 2D data can be easily displayed as points in a cartesian plane. Our dataset forms 6 petals of a stylised flower, half red (class label $y=0$), half blue (class label $y=1$). The goal is to build a model to classify cartesian coordinates in the 2D plane corresponding to the blue or red petal class.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "## B. Logistic Regression Model ##\n",
    "\n",
    "In our previous notebook, we learned how to build and train a logistic regression model to fit our dataset. It performed relatively well on our dataset of images. Let's try and fit our data with the same model. Rather than using our previous implementation, we'll use scikit `sklearn` built-in functions to perform the regression. \n",
    "\n",
    "Evaluate the following code which defines a function useful to visualise the decision boundaries for a given model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model, X, Y):\n",
    "    range0 = np.arange(X[:, 0].min() - 0.1, X[:, 0].max() + 0.1, 0.05)\n",
    "    range1 = np.arange(X[:, 1].min() - 0.1, X[:, 1].max() + 0.1, 0.05)\n",
    "    XX, YY = np.meshgrid(range0, range1)\n",
    "    Y_hat = model(np.c_[XX.ravel(), YY.ravel()])\n",
    "    Y_hat = Y_hat.reshape(XX.shape)\n",
    "    plt.contourf(XX, YY, Y_hat, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('$x_2$')\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='darkgreen'>**Exercise 2:**</font> Complete and evaluate the following cell to report our model accuracy and visualise the model classification output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and train Logistic regression model\n",
    "model = sklearn.linear_model.LogisticRegressionCV(cv=3);\n",
    "model.fit(X_train, Y_train.ravel());\n",
    "# Visualise class regions and decision boundaries\n",
    "plot_model(lambda x: model.predict(x), X_test, Y_test.reshape(-1))\n",
    "plt.title(\"Logistic Regression (test data)\")\n",
    "\n",
    "# Report model performance on training and test data\n",
    "Y_hat_train = model.predict(X_train).reshape(Y_train.shape)\n",
    "Y_hat_test = model.predict(X_test).reshape(Y_test.shape)\n",
    "n_train = Y_train.shape[0]\n",
    "n_test = Y_test.shape[0]\n",
    "\n",
    "### INPUT YOUR CODE HERE ### (2 lines)\n",
    "train_acc = None\n",
    "test_acc = None\n",
    "### END OF YOUR CODE SEGMENT ###  \n",
    "\n",
    "print(\"model accuracy (training) = {:.1f}%\".format(train_acc))\n",
    "print(\"testing model (test)= {:.1f}%\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    "> \n",
    "> ```\n",
    "> model accuracy (training) = 50.8%\n",
    "> testing model (test)= 49.2%\n",
    "> ```\n",
    "\n",
    "A logistic regression model fails to separate the non-linear dependencies in our dataset. We will now construct a simple single layer neural network to better capture our model complexity.\n",
    "\n",
    "***\n",
    "\n",
    "## C. Single Layer Feedforward Neural Network Model ##\n",
    "\n",
    "We can define a binary single layer feedforward neural network with $n$ hidden units for our two class dataset as follows:\n",
    "\n",
    "<img src=\"figs/SLFN.png\" style=\"width:600px\">\n",
    "\n",
    "Here we choose the $\\tanh(\\cdot)$ activation function for the hidden unit. Given a single input data $x$ (row vector), the model can be summarised with the following set of equations:\n",
    "\n",
    "$$z^{[1]} = x\\, W^{[1]} + b^{[1]}\\notag$$ \n",
    "$$a^{[1]} = \\tanh(z^{[1]})\\notag$$\n",
    "$$z^{[2]} = a^{[1]}\\, W^{[2]} + b^{[2]}\\notag$$\n",
    "$$a^{[2]} = \\sigma(z^{ [2]})\\in (0,1)\\notag$$\n",
    "$$\\hat{y} = \\mathbb 1_{a^\\text{[2]}> 1/2}$$\n",
    "\n",
    "where $\\theta=(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]})$ are the model parameters for the hidden and output layers. Just like for the logistic regression, the cross-entropy loss is obtained by: \n",
    "$$\\ell(y, \\hat{y}) =  -y\\log(\\hat{y}) - (1-y)\\log(1-\\hat{y})\\notag$$\n",
    "\n",
    "The overall loss for $n_{train}$ training examples is computed from the sum of all individual losses:\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{n_{train}} \\sum_{i=1}^{n_{train}} \\ell(y^{(i)}, \\hat{y}^{(i)})\\notag$$\n",
    "\n",
    "To implement the single layer feedforward neural network using backprop, we will take the following steps:\n",
    "\n",
    "- Define the network topology (in our case this is just the number of hidden units $n$)\n",
    "- Initialise the model weights\n",
    "- Evaluate loss for the model prediction using foward propagation\n",
    "- Compute gradients for loss\n",
    "- Train the model by updating parameters using backprop\n",
    "- Evaluate performance the model on the test dataset\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 3:**</font> Setup model topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config(X, Y, hidden_units=3):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- n data samples, shape = (n, n_x)\n",
    "    Y -- ground truth label, column vector of shape (n, 1)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- number of units in the input layer\n",
    "    n_y -- number of units in the output layer\n",
    "    n_h -- number of units in the hidden layer\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (3 lines)\n",
    "    n_x = None\n",
    "    n_h = None\n",
    "    n_y = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    assert(X.shape[0] == Y.shape[0])\n",
    "    return (n_x, n_y, n_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, hidden_units = t3.model_config_test()\n",
    "(n_x, n_y, n_h) = model_config(X, Y, hidden_units)\n",
    "\n",
    "print(\"{} input units\".format(n_x))\n",
    "print(\"{} output unit\".format(n_y))\n",
    "print(\"{} hidden units\".format(n_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> 2 input units\n",
    "> 1 output unit\n",
    "> 8 hidden units\n",
    "> ```\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 4:**</font> Initialise the model weights to random values drawn from $\\mathcal{N}(0,0.01)$ and set bias to zeros. You can use `np.random.randn(n,m) * 0.01` to randomly initialize a matrix of shape $(n,m)$ and `np.zeros((n,m))` to initialize a matrix of shape $(n,m)$ with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameter initialisation\n",
    "def init_model_parameters(n_x, n_y, n_h):\n",
    "    \"\"\"\n",
    "    n_x -- number of units in the input layer\n",
    "    n_y -- number of units in the output layer\n",
    "    n_h -- number of units in the hidden layer\n",
    "    \n",
    "    Returns: dictionary containing your parameters:\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h)\n",
    "        b1 -- initialised bias vector of shape (1, n_h)\n",
    "        W2 -- initialised weight matrix of shape (n_h, n_y)\n",
    "        b2 -- initialised bias vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (4 lines)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "\n",
    "    assert(W1.shape == (n_x, n_h))\n",
    "    assert(b1.shape == (1, n_h))\n",
    "    assert(W2.shape == (n_h, n_y))\n",
    "    assert(b2.shape == (1, n_y))\n",
    "    \n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "params = init_model_parameters(n_x=2, n_y=1, n_h=3)\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "print(\"W1 = {}\".format(params[\"W1\"]))\n",
    "print(\"b1 = {}\".format(params[\"b1\"]))\n",
    "print(\"W2.T = {}\".format(params[\"W2\"].T))\n",
    "print(\"b2 = {}\".format(params[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> W1 = [[-0.00218  0.00821  0.01481]\n",
    ">      [ 0.01332 -0.00362  0.00686]]\n",
    "> b1 = [[0. 0. 0.]]\n",
    "> W2.T = [[ 0.00574  0.00288 -0.00236]]\n",
    "> b2 = [[0.]]\n",
    "> ```\n",
    ">\n",
    "> <font color='darkgreen'>**Exercise 5:**</font> Compute the forward propagation $a^{[2]}$ along with a cache dictionary for the values of $z^{[1]}, a^{[1]}, z^{[2]}$. Also return the loss as defined in the equations above if the ground truth parameter is available. Cached values are required to compute partial derivatives for the backprop algorithm. Use `np.tanh()` and `t3.sigmoid()` to compute the activations of the hidden and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation (inference)\n",
    "def forward_prop(params, X, Y=None):\n",
    "    \"\"\"\n",
    "    Compute the layer activations and loss if needed\n",
    "\n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h)\n",
    "        b1 -- initialised bias vector of shape (1, n_h)\n",
    "        W2 -- initialised weight matrix of shape (n_h, n_y)\n",
    "        b2 -- initialised bias vector of shape (1, n_y)\n",
    "    X -- n data samples, shape = (n, n_x)\n",
    "    Y -- optional argument, ground truth label, column vector of shape (n, 1)\n",
    "\n",
    "    Returns:\n",
    "    loss -- cross-entropy loss or NaN if Y=None\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\", A2\"\n",
    "        Z1 -- matrix of shape (n, n_h)\n",
    "        A1 -- matrix of shape (n, n_h)\n",
    "        Z2 -- matrix of shape (n, n_y)\n",
    "        A2 -- matrix of shape (n, n_y)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Retrieve model parameters\n",
    "    ### INPUT YOUR CODE HERE ### (4 lines)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    # Forward prop: compute cache from X\n",
    "    ### INPUT YOUR CODE HERE ### (4 lines)\n",
    "    Z1 = None\n",
    "    A1 = None\n",
    "    Z2 = None\n",
    "    A2 = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    n_x = params[\"W1\"].shape[0]\n",
    "    n_y = params[\"W2\"].shape[1]\n",
    "    n_h = params[\"W1\"].shape[1]\n",
    "    assert(A1.shape == (n, n_h))\n",
    "    assert(Z1.shape == (n, n_h))\n",
    "    assert(A2.shape == (n, n_y))\n",
    "    assert(Z2.shape == (n, n_y))\n",
    "    \n",
    "    loss = float('nan')\n",
    "    if Y is not None:\n",
    "        Y_hat = A2\n",
    "        # Compute the cross-entropy loss\n",
    "        ### INPUT YOUR CODE HERE ### (1 line)\n",
    "        loss = None\n",
    "        ### END OF YOUR CODE SEGMENT ###  \n",
    "\n",
    "        loss = np.squeeze(loss)\n",
    "        assert(loss.dtype == float)\n",
    "        \n",
    "    return {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, X, Y = t3.forward_prop_test()\n",
    "cache, loss = forward_prop(params, X, Y)\n",
    "\n",
    "print(\"np.mean(Z1) = {:.5f}\".format(np.mean(cache[\"Z1\"])))\n",
    "print(\"np.mean(A1) = {:.5f}\".format(np.mean(cache[\"A1\"])))\n",
    "print(\"np.mean(Z2) = {:.5f}\".format(np.mean(cache[\"Z2\"])))\n",
    "print(\"np.mean(A2) = {:.5f}\".format(np.mean(cache[\"A2\"])))\n",
    "print(\"loss = {:.5f}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> np.mean(Z1) = -0.04107\n",
    "> np.mean(A1) = 0.03057\n",
    "> np.mean(Z2) = -1.80453\n",
    "> np.mean(A2) = 0.15003\n",
    "> loss = 1.26159\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "To train the model, we perform a gradient descent by updating our parameters, $\\theta \\equiv (W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]})$, iteratively using backprop. This optimisation step requires to compute the loss over the training set for the current model parameters $\\theta$ and then update their values according to the following rule:  $ \\theta = \\theta - \\lambda\\, d\\theta$ where $\\lambda$ is an hyper-parameters scalar called the learning rate and $d\\theta=\\frac{\\partial\\mathcal{L}}{\\partial\\theta}$. This is repeated a number of iterations as described by another hyper-parameters referred to as the epoch count or epochs (1 epoch = complete presentation of training data).\n",
    "\n",
    "Using the cache computed during forward propagation, you can now implement the backward propagation step.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 6:**</font> Implement the function `back_prop()`<br>\n",
    ">\n",
    "> For a single input $x$, a summary of the partial derivatives required to implement backprop for our single layer feedfoward neural network are given below:\n",
    ">\n",
    "> $$dz^{[2]}\\equiv\\frac{\\partial \\ell}{\\partial z^{[2]}} = \\frac{\\partial \\ell}{\\partial a^{[2]}}\\frac{\\partial a^{[2]}}{\\partial z^{[2]}} = \\frac{\\partial \\ell}{\\partial a^{[2]}}\\sigma'(z^{[2]})= \\left(\\frac{-y}{a^{[2]}}+\\frac{1-y}{1-a^{[2]}}\\right)\\left(a^{[2]}(1-a^{[2]})\\right)= a^{[2]}-y \\notag$$\n",
    ">\n",
    "> $$dW^{[2]}\\equiv\\frac{\\partial \\ell}{\\partial W^{[2]}} = a^{[1]T} dz^{[2]} $$\n",
    "> $$db^{[2]}\\equiv\\frac{\\partial \\ell}{\\partial b^{[2]}} = dz^{[2]}$$\n",
    "> \n",
    "> $$dz^{[1]}\\equiv\\frac{\\partial \\ell}{\\partial z^{[1]}} = \\frac{\\partial \\ell}{\\partial z^{[2]}}\\frac{\\partial z^{[2]}}{\\partial a^{[1]}}\\frac{\\partial a^{[1]}}{\\partial z^{[1]}} = \\left(dz^{[2]}\\,w^{[2]T}\\right)\\odot\\tanh'(z^{[1]})\\notag$$\n",
    ">\n",
    "> $$dW^{[1]}\\equiv\\frac{\\partial \\ell}{\\partial W^{[1]}} = x^T\\,dz^{[1]}$$\n",
    "> $$db^{[1]}\\equiv\\frac{\\partial \\ell}{\\partial b^{[1]}} = dz^{[1]}$$\n",
    ">\n",
    "> Note, $\\odot$ denotes the element wise product.\n",
    ">\n",
    "> For a $n$ samples stacked in rows in matrix $X$, a vectorised expression for the partial derivatives is given as follows:\n",
    "> \n",
    "> $$dZ^{[2]}= A^{[2]}-Y \\tag{1}$$\n",
    ">\n",
    "> $$dW^{[2]}=\\frac{1}{n} A^{[1]T}\\, dZ^{[2]} \\tag{2}$$\n",
    "> $$db^{[2]}_j=\\frac{1}{n} \\sum_{i=1}^{n} dZ^{[2]}_{i,j} \\tag{3}$$\n",
    ">\n",
    "> $$dZ^{[1]}=\\left(dZ^{[2]}\\,W^{[2]T}\\right)\\odot\\tanh'(Z^{[1]})=\n",
    "% \\left(dZ^{[2]}\\,W^{[2]T}\\right)\\odot\\left(1-\\tanh(Z^{[1]})^2\\right)=\n",
    "\\left(dZ^{[2]}\\,W^{[2]T}\\right)\\odot\\left(1-{A^{[1]}}^2\\right)\n",
    "\\tag{4}$$\n",
    ">\n",
    "> $$dW^{[1]}=\\frac{1}{n}   X^T \\, dZ^{[1]}\\tag{5}$$\n",
    "> $$db^{[1]}_j=\\frac{1}{n} \\sum_{i=1}^{n}  dZ^{[1]}_{i,j} \\tag{6}$$\n",
    ">\n",
    "> Use equations (1-6) to implement the `back_prop()` function using the cache containing layers' activations. This step is generally the most challenging task in a DL model implementation as it relies on differential calculus. Checking matrix dimensions generally helps debugging most situations. Note `np.sum(A, axis=0, keepdims=True)` computes the sum along the first dimension of matrix $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward_propagation\n",
    "def back_prop(params, X, Y, cache):\n",
    "    \"\"\"\n",
    "    Compute back-propagation gradients\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h)\n",
    "        b1 -- initialised bias vector of shape (1, n_h)\n",
    "        W2 -- initialised weight matrix of shape (n_h, n_y)\n",
    "        b2 -- initialised bias vector of shape (1, n_y)\n",
    "    X -- n data samples, shape = (n, n_x)\n",
    "    Y -- ground truth label, column vector of shape (n, 1)\n",
    "    cache -- dictionary containing \"Z1\", \"A1\", \"Z2\", A2\"\n",
    "        Z1 -- matrix of shape (n, n_h)\n",
    "        A1 -- matrix of shape (n, n_h)\n",
    "        Z2 -- matrix of shape (n, n_y)\n",
    "        A2 -- matrix of shape (n, n_y)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- dictionary containing your gradients with respect to all parameters\n",
    "        dW1 -- weight gradient matrix of shape (n_x, n_h)\n",
    "        db1 -- bias gradient vector of shape (1, n_h)\n",
    "        dW2 -- weight gradient matrix of shape (n_h, n_y)\n",
    "        db2 -- bias gradient vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # Retrieve w1 and w2 weights from params dictionary\n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    W1 = None\n",
    "    W2 = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "        \n",
    "    # Retrieve A1 and A2 from cache dictionary\n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    A1 = None\n",
    "    A2 = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    # Backprop calculation for dw1, db1, dw2, db2\n",
    "    ### INPUT YOUR CODE HERE ### (6 lines)\n",
    "    dZ2 = None\n",
    "    dW2 = None\n",
    "    db2 = None\n",
    "    dZ1 = None\n",
    "    dW1 = None\n",
    "    db1 = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "        \n",
    "    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, X, Y, cache = t3.back_prop_test()\n",
    "grads = back_prop(params, X, Y, cache)\n",
    "\n",
    "print(\"dW1 = {}\".format(grads[\"dW1\"]))\n",
    "print(\"db1 = {}\".format(grads[\"db1\"]))\n",
    "print(\"dW2.T = {}\".format(grads[\"dW2\"].T))\n",
    "print(\"db2 = {}\".format(grads[\"db2\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> dW1 = [[ 0.08191  0.03541  0.0049   0.00276 -0.1456 ]\n",
    ">        [-0.12162 -0.12366  0.18207 -0.12262 -0.09771]]\n",
    "> db1 = [[-0.17377 -0.15197  0.21951 -0.13308 -0.04902]]\n",
    "> dW2.T = [[0.37147 0.27293 0.08623 0.40718 0.38098]]\n",
    "> db2 = [[-0.49222]]\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 7:**</font> Implement the update rule in function `update_params()` <br>\n",
    ">\n",
    "> The update rule for gradient descent is given $\\theta = \\theta - \\lambda\\, d\\theta$ where $\\lambda$ denotes the learning rate. The choice for the value of hyper-parameters $\\lambda$ is important, a small value will lead to slow convergence while a value too large may result in divergence.\n",
    ">\n",
    "> In our case our model parameters $\\theta$ describe the tuple\n",
    "$(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model parameters\n",
    "def update_params(params, grads, learning_rate=0.8):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- weight matrix of shape (n_x, n_h)\n",
    "        b1 -- bias vector of shape (1, n_h)\n",
    "        W2 -- weight matrix of shape (n_h, n_y)\n",
    "        b2 -- bias vector of shape (1, n_y)\n",
    "    grads -- dictionary containing gradients\n",
    "        dW1 -- weight gradient matrix of shape (n_x, n_h)\n",
    "        db1 -- bias gradient vector of shape (1, n_h)\n",
    "        dW2 -- weight gradient matrix of shape (n_h, n_y)\n",
    "        db2 -- bias gradient vector of shape (1, n_y)\n",
    "    learning_rate -- learning rate of the gradient descent (hyper-parameters)\n",
    "\n",
    "    Returns:\n",
    "    params -- dictionary containing updated parameters\n",
    "        W1 -- updated weight matrix of shape (n_x, n_h)\n",
    "        b1 -- updated bias vector of shape (1, n_h)\n",
    "        W2 -- updated weight matrix of shape (n_h, n_y)\n",
    "        b2 -- updated bias vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    dW1 = None\n",
    "    db1 = None\n",
    "    dW2 = None\n",
    "    db2 = None\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, grads = t3.update_params_test()\n",
    "params = update_params(params, grads)\n",
    "\n",
    "print(\"W1 = {}\".format(params[\"W1\"]))\n",
    "print(\"b1 = {}\".format(params[\"b1\"]))\n",
    "print(\"W2.T = {}\".format(params[\"W2\"].T))\n",
    "print(\"b2 = {}\".format(params[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> W1 = [[-0.26965  3.38549  0.6373   0.68604 -0.74118]\n",
    ">       [ 0.35687  0.18441  0.71615  0.43549  1.60339]]\n",
    "> b1 = [[-1.32899 -1.20158 -0.15703  0.4257   0.9854 ]]\n",
    "> W2.T = [[ 0.60585  2.20273 -0.38625  0.65574  0.1165 ]]\n",
    "> b2 = [[-0.18005]]\n",
    ">```\n",
    "\n",
    "*** \n",
    "\n",
    "> <font color='darkgreen'>**Exercise 8:**</font> Complete the function below to train your single layer feedforward neural network model by repeatedly performing inference, computing parameter gradients using backprop, and updating parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter optimisation using backprop\n",
    "def model_fit(params, X, Y, epochs=2000, learning_rate=0.8, verbose=False):\n",
    "    \"\"\"\n",
    "    Optimise model parameters performing gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h)\n",
    "        b1 -- initialised bias vector of shape (1, n_h)\n",
    "        W2 -- initialised weight matrix of shape (n_h, n_y)\n",
    "        b2 -- initialised bias vector of shape (1, n_y)\n",
    "    X -- n data samples  (n, n_x)\n",
    "    Y -- ground truth label vector of size (n, n_y)\n",
    "    epochs -- number of iteration updates through dataset\n",
    "    learning_rate -- learning rate of the gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary with optimised parameters\n",
    "    grads -- dictionary with final gradients\n",
    "    loss_log -- list of loss values for every 1000 updates\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_log = []\n",
    "    for i in range(epochs):\n",
    "        ### INPUT YOUR CODE HERE ### (5 lines)\n",
    "        cache, loss = None\n",
    "        grads = None\n",
    "        params = None\n",
    "        ### END OF YOUR CODE SEGMENT ###  \n",
    "        \n",
    "        # logs\n",
    "        if i % 1000 == 0:\n",
    "            loss_log.append(loss.item())\n",
    "            if verbose:\n",
    "                print(\"Loss after {} iterations: {:.3f}\".format(i, loss))\n",
    "     \n",
    "    return params, grads, loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, X, Y = t3.forward_prop_test()\n",
    "params, grads, loss_log = model_fit(params, X, Y, epochs = 2100, verbose=True)\n",
    "\n",
    "print(\"W1 = {}\".format(params[\"W1\"]))\n",
    "print(\"b1 = {}\".format(params[\"b1\"]))\n",
    "print(\"W2.T = {}\".format(params[\"W2\"].T))\n",
    "print(\"b2 = {}\".format(params[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> Loss after 0 iterations: 1.262\n",
    "> Loss after 1000 iterations: 0.002\n",
    "> Loss after 2000 iterations: 0.001\n",
    "> W1 = [[ 5.82809 -4.5018   0.15839  0.02176 -0.62088]\n",
    ">       [ 3.95058 -1.81251  2.21641  0.47214  0.75909]]\n",
    "> b1 = [[-1.26981  0.0359  -0.83395 -3.19964  1.01775]]\n",
    "> W2.T = [[ 7.87515 -4.7095   1.23063  0.18959 -0.29637]]\n",
    "> b2 = [[-0.52574]]\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "Once the model parameters are optimised, we can use our single layer feedforward neural network  model to predict the petal class for our dataset `X_test`. \n",
    "\n",
    "> <font color='darkgreen'>**Exercise 9:**</font> Implement the `model_predict()` function<br>\n",
    ">\n",
    "> Perform model inference on input $X$ and convert activation output to predictions (0/1 values): \n",
    "> \n",
    "> $\\hat{y}=\\mathbb 1_{a^{[2]}> 1/2} \\equiv \\begin{cases}\n",
    "      1 & \\text{if } a^{[2]}> 1/2 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  \n",
    ">    \n",
    "> Avoid python iteration and `if` statements preferring numpy vectorial code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model inference\n",
    "def model_predict(params, X):\n",
    "    '''\n",
    "    Predict class label using model parameters\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- optimised weight matrix of shape (n_x, n_h)\n",
    "        b1 -- optimised bias vector of shape (1, n_h)\n",
    "        W2 -- optimised weight matrix of shape (n_h, n_y)\n",
    "        b2 -- optimised bias vector of shape (1, n_y)\n",
    "    X -- n data samples  (n, n_x)\n",
    "    \n",
    "    Returns:\n",
    "    Y_hat -- vector with class predictions for examples in X\n",
    "    '''\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    cache, _ = None\n",
    "    Y_hat = None # Convert activations to {0,1} predictions\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    n = X.shape[0]\n",
    "    assert(Y_hat.shape == (n, 1))    \n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, X = t3.model_predict_test()\n",
    "Y_hat = model_predict(params, X)\n",
    "\n",
    "print(\"predictions.T = {}\".format(Y_hat.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `predictions.T = [[1 1 0]]`\n",
    "\n",
    "***\n",
    "\n",
    "## D. Stitching it all together ##\n",
    "\n",
    "We can now fit all the previous step into a model function that will take our training and testing datasets as input along with our two hyper-parameters (learning rate and number of epochs for training) and that will return trained model parameters along training and testing logs useful to study the training behaviour of our backprop optimisation. \n",
    "\n",
    "> <font color='darkgreen'>**Exercise 10:**</font> Implement the following function and evaluate your model on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLFN model\n",
    "def slfn_model(X_train, Y_train, X_test, Y_test, hidden_units=3, epochs=10000, learning_rate=0.5):\n",
    "    '''\n",
    "    Build, train and evaluate the logistic regression model\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set a numpy array of shape (n_train, n_x)\n",
    "    Y_train -- training ground truth vector (0=dog, 1=cat) of size (n_train, n_y)\n",
    "    X_test -- testing set a numpy array of shape (n_test, n_x)\n",
    "    Y_test -- testing ground truth vector (0=dog, 1=cat) of size (n_test, n_y)\n",
    "    hidden_units -- number of units in hidden layer\n",
    "    epochs -- number of iteration updates through dataset for training (hyper-parameters)\n",
    "    learning_rate -- learning rate of the gradient descent (hyper-parameters)\n",
    "    \n",
    "    Returns:\n",
    "    model -- dictionary \n",
    "        PARAMS -- parameters\n",
    "        LOSS -- log of training loss\n",
    "        GRADS -- final \n",
    "        ACC -- array with training and testing accuracies\n",
    "        LR -- learning rate\n",
    "    '''\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (7 lines)\n",
    "    (n_x, n_y, n_h) = None\n",
    "    params = None\n",
    "    params, grads, loss = None\n",
    "    Y_hat_train = None\n",
    "    Y_hat_test = None\n",
    "    train_acc = None\n",
    "    test_acc = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "\n",
    "    print(\"{:.1f}% training acc.\".format(train_acc))\n",
    "    print(\"{:.1f}% test acc.\".format(test_acc))\n",
    "        \n",
    "    return {\"PARAMS\": params, \"LOSS\": loss, \"GRADS\": grads, \"ACC\": [train_acc, test_acc], \"LR\": learning_rate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = slfn_model(X_train, Y_train, X_test, Y_test)\n",
    "params = model[\"PARAMS\"]\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_model(lambda x: model_predict(params, x), X_test, Y_test.reshape(-1))\n",
    "_ = plt.title(\"SLFN with {} hidden units\".format(params[\"W1\"].shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> 94.5% training acc.\n",
    "> 91.0% test acc.\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "Accuracy is much improved compared to the logistic regression model. With 3 hidden units, our model seems to have captured the essence of the two-class petals. Single layer feedforward neural network are able to learn highly non-linear detection regions which is not possible with simple logistic regression models.\n",
    "\n",
    "Evaluate the following code to experiment with the topology hyper-parameter that determines the number of hidden units, sometimes referred to as the network capacity. It may take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 32))\n",
    "for i, hidden_units in enumerate([1, 2, 3, 4, 8, 16]):\n",
    "    print(\"SLFN with {} hidden units\".format(hidden_units))\n",
    "    model = slfn_model(X_train, Y_train, X_test, Y_test, hidden_units)\n",
    "    params = model[\"PARAMS\"]\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    plot_model(lambda x: model_predict(params, x), X_test, Y_test.reshape(-1))\n",
    "    _ = plt.title(\"SLFN with {} hidden units\".format(hidden_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Larger models with more hidden units are able to fit the training better but eventually the largest models overfit the data (training accuracy still going up while testing accuracy starts decreasing)\n",
    "- It is possible to use larger models and prevent overfitting with a technique called regularisation.\n",
    "- For our datasets, the best hidden layer size appears to be around 3 hidden units with little noticeable overfitting.\n",
    "\n",
    "***\n",
    "\n",
    "Scikit provides a few 2D datasets that you may want to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [sklearn.datasets.make_circles(n_samples=512, factor=.5, noise=.1), \n",
    "            sklearn.datasets.make_moons(n_samples=512, noise=.3),\n",
    "            sklearn.datasets.make_gaussian_quantiles(mean=None, cov=0.3, n_samples=512, n_features=2, n_classes=2, shuffle=True, random_state=None)]\n",
    "index = int(np.random.randint(len(datasets), size=1))\n",
    "X, Y = datasets[index]\n",
    "Y = Y.reshape(Y.shape[0], 1)\n",
    "X_train= X[0:256, :]\n",
    "X_test= X[256:512, :]\n",
    "Y_train= Y[0:256, :]\n",
    "Y_test= Y[256:512, :]\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train.reshape(-1), s=20, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = slfn_model(X_train, Y_train, X_test, Y_test)\n",
    "params = model[\"PARAMS\"]\n",
    "\n",
    "# Plot the decision boundary\n",
    "plot_model(lambda x: model_predict(params, x), X_test, Y_test.reshape(-1))\n",
    "_ = plt.title(\"SLFN with {} hidden units\".format(params[\"W1\"].shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- EOF --"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "ml-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
