{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python & Numpy #\n",
    "\n",
    "Objectives: familiarize yourself with Python 3 concepts, focusing on replacing loops with efficient, vectorized operations using the numpy library.\n",
    "\n",
    "> **Instructions:** <br>\n",
    "> - Verify that your Python environment is correctly set up with Jupyter installed.\n",
    "> - As we progress through this module, you may be required to install additional packages.\n",
    "> - Prior to this tutorial, review the entire notebook and try all coding exercises.\n",
    "> - To complete the coding exercises, insert your Python code between<br>`### INPUT YOUR CODE HERE ###` and `### END OF YOUR CODE SEGMENT ###`\n",
    ">-  We will generally provide a rough estimate of the number of lines to write.\n",
    "> - Execute cells using `Shift+Enter` and ensure your results match the provided unit tests.\n",
    "\n",
    "By completing this tutorial, you will understand numpy vectorization.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 1:**</font> Test your Python 3 runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "### INPUT YOUR CODE HERE ### (1 line)\n",
    "test = \"Hello DL!\"\n",
    "### END OF YOUR CODE SEGMENT ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log result: Hello DL!\n"
     ]
    }
   ],
   "source": [
    "print(\"log result: \" + test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected output:***\n",
    ">\n",
    "> `log result: Hello DL!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## A. Numpy ##\n",
    "\n",
    "[Numpy](http://www.numpy.org) is a highly optimized scientific computing library for Python. Many math functions in numpy accept arrays as input, for example `np.exp(.)` is the numpy equivalent of `math.exp(.)`. You will now implement a function to apply the sigmoid function $\\sigma(x) = \\frac{1}{1+e^{-x}}$ to an array of scalars. This symmetric function is highly non-linear and maps scalar values to the interval (0,1). It is also referred to as the logistic function and is frequently used in machine learning and deep learning. You will use it in the next tutorial to carry out a simple regression classification task on images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x=(x_1, x_2, \\dots, x_n)$ be a row vector then `np.exp(x)` will apply the exponential function to every element of $x$, with the resulting vector $(e^{x_1}, e^{x_2}, \\ldots, e^{x_n})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.   2.72 7.39]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "x = np.array([0, 1, 2])\n",
    "print(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, if $x$ is a vector then a Python operation such as $\\frac{1}{2 x + 1}$ produces a vector of the same size as $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10.   2. -10.]\n"
     ]
    }
   ],
   "source": [
    "# Perform numpy vector operations\n",
    "x = np.array([0, 0.2, -0.1])\n",
    "print(1 / (2 * x + 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='darkgreen'>**Exercise 2:**</font> Implement the sigmoid function using numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # for easy access to numpy functions\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "# Define the sigmoid function using numpy\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- scalar or numpy array of any size\n",
    "\n",
    "    Returns:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (1 line)\n",
    "    s = (np.exp(x)/(1+np.exp(x)))\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.73105858, 0.88079708])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.array([0, 1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `array([0.5 , 0.73, 0.88])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.arange(-8, 8, 0.1)\n",
    "plt.plot(x,sigmoid(x))\n",
    "_ = plt.title('sigmoid(.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back-propagation in loss function optimization relies on gradient computation. This is the backbone of a training process in deep learning. \n",
    "\n",
    "> <font color='darkgreen'>**Exercise 3:**</font> Let's now code the gradient function for the sigmoid map given by $\\frac{d}{dx} \\sigma(x) =\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def grad_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the sigmoid function with respect to its input x\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Returns:\n",
    "    ds -- Your computed gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    s = None\n",
    "    ds = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"grad_sigmoid = \" + str(grad_sigmoid(np.array([1, 2, 3]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `grad_sigmoid = [0.2  0.1  0.05]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(-8, 8, 0.1)\n",
    "plt.plot(x,grad_sigmoid(x),'-')\n",
    "_ = plt.title('sigmoid\\'(.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### A.1 Reshaping Arrays ###\n",
    "\n",
    "Another very common operation in deep learning is the manipulation of arrays. For example, converting a tensor input of dimension $(W,H,D)$ to a column vector of size $(W\\times H\\times D, 1)$. Such unrolling operation is very frequent and referred to as reshaping. Numpy provides two useful functions for these tasks:\n",
    "\n",
    "- `np.shape`: to query the dimension\n",
    "- `np.reshape()`: to reinterpret array with different dimensions\n",
    "\n",
    "<img src=\"figs/reshape.png\" alt=\"Image Reshape\" width=\"500\"/>\n",
    "\n",
    "An image is typically represented as a 3D tensor of shape $(W,H,D)$ which may need to be rearranged as vector of $(W\\times H\\times D, 1)$ elements.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 4:**</font> Implement the function `im2vec()` that takes an input of shape $(W, H, 3)$ and returns a column vector of shape $(W\\times H\\times 3,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def im2vec(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (W, H, D)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a column vector of shape (W*H*D, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (1 line)\n",
    "    v = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Example of a 2x2 RGB image\n",
    "image = np.array([\n",
    "    [[ 0.2, 0.1, 0.8],\n",
    "     [ 0.8, 0.7, 0.5]],\n",
    "    \n",
    "    [[ 0.4, 0.2, 0.7],\n",
    "     [ 0.5, 0.3, 0.2]]])\n",
    "\n",
    "print(\"image.shape = {}\".format(image.shape))\n",
    "print(\"im2vec(image).shape = {}\".format(im2vec(image).shape))\n",
    "print(\"im2vec(image).T = {}\".format(im2vec(image).T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `image.shape = (2, 2, 3)`<br/>\n",
    "> `im2vec(image).shape = (12, 1)`<br/>\n",
    "> `im2vec(image).T = [[0.2 0.1 0.8 0.8 0.7 0.5 0.4 0.2 0.7 0.5 0.3 0.2]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### A.2 Normalisation of Data ###\n",
    "\n",
    "Normalisation of data is an important preprocessing step in deep learning. It often yields better performance by accelerating convergence and better optimisation. A typical strategy consists in setting input vectors to unit value. Given an input vector $x$, the normalised vector is given by $\\frac{x}{\\|x\\|}$.\n",
    "\n",
    "If your data samples are organised as row or column entries in a matrix, you can use `np.linalg.norm()` and specify the axis along which you want to compute the norm. For example:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "    1 & 2 & 3 \\\\\n",
    "    4 & 5 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "then \n",
    "\n",
    "$$\n",
    "\\|X\\| = \n",
    "\\texttt{np.linalg.norm(X, axis=1, keepdims=True)} = \\begin{bmatrix}\n",
    "    \\sqrt{15} \\\\\n",
    "    \\sqrt{77} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "X'=\\frac{X}{\\|X\\|}=\\begin{bmatrix}\n",
    "    \\frac{1}{\\sqrt{15}} & \\frac{2}{\\sqrt{15}} & \\frac{3}{\\sqrt{15}} \\\\\n",
    "    \\frac{4}{\\sqrt{77}} & \\frac{5}{\\sqrt{77}} & \\frac{6}{\\sqrt{77}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that although $X$ is a $2\\times 3$ matrix and $\\| X\\|$ is $2\\times 1$,\n",
    "numpy permits the division of $X$ by $\\| X\\|$ using a mechanism called [broadcasting](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html). It is useful when performing operations between arrays of different shapes.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 5:**</font> Implement `normalise()` such that each row of the input X are unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def normalise(X):\n",
    "    \"\"\"\n",
    "    Normalises each row of the matrix X to unit length\n",
    "    \n",
    "    Argument:\n",
    "    X -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    X -- The modified and normalised numpy matrix X\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    X_norm = None\n",
    "    X = None\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(\"normalised X.shape = {}\".format(normalise(X).shape))\n",
    "print(\"normalised X = {}\".format(normalise(X)))\n",
    "assert(np.sum(np.linalg.norm(normalise(X),axis=1))==2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">***Expected Output:***\n",
    ">\n",
    "> `normalised X.shape = (2, 3)` <br/>\n",
    "> `normalised X = [[0.27 0.53 0.8 ]` <br/>\n",
    "> `[0.46 0.57 0.68]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important normalisation function used in multi-dimensional classification problems is called the softmax function. Formally if $x\\in \\mathbb{R}^{1\\times n}$, the softmax function is defined as\n",
    "\n",
    "$$\\texttt{softmax}(x) = \\left[\\frac{e^{x_1}}{\\sum_{i=1}^n e^{x_i}},\\frac{e^{x_2}}{\\sum_{i=1}^n e^{x_i}},\\ldots,\\frac{e^{x_n}}{\\sum_{i=1}^n e^{x_i}}\\right]$$ \n",
    "\n",
    "> <font color='darkgreen'>**Exercise 6:**</font> Implement the softmax function using numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    \"\"\"Softmax for each row of the input X of shape (n, m)\n",
    "\n",
    "    Argument:\n",
    "    X -- A numpy matrix of shape (n,m)\n",
    "\n",
    "    Returns:\n",
    "    S -- A numpy matrix of shape (n,m) equal to the softmax of X\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (3 lines)\n",
    "    X_exp = None\n",
    "    X_sum = None\n",
    "    S = None\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.array([[3, 0, 6],[8, 8, 0]])\n",
    "print(\"softmax = \\n {}\".format(softmax(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">***Expected Output:***\n",
    ">\n",
    ">`softmax = ` <br/>\n",
    "> `[[0.05 0.   0.95]` <br/>\n",
    "> `[0.5  0.5  0.  ]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "- You could do a sanity check and verify that each row sums up to 1 with numpy function `np.sum(softmax(X),1)`\n",
    "- The softmax function lets us treat each row element as a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "***\n",
    "\n",
    "## B. Vectorisation ##\n",
    "\n",
    "Training deep learning models is generally performed on large datasets. To avoid severe performance bottlenecks, you must ensure your code runs efficiently. This is particularly important when using scripting languages such as Python where compiler optimisation is not available. One very important concept is called vectorisation. Evaluate and study the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "x = [3, 8, 5, 0, 3, 9, 4, 0]\n",
    "y = [1, 8, 2, 7, 4, 5, 7, 2]\n",
    "\n",
    "### python outer product ###\n",
    "delta_t1 = time.time()\n",
    "for k in range(1000):\n",
    "    outer1 = np.zeros((len(x),len(y))) \n",
    "    for i in range(len(x)):\n",
    "        for j in range(len(y)):\n",
    "            outer1[i,j] = x[i] * y[j]\n",
    "delta_t1 = time.time() - delta_t1\n",
    "\n",
    "### Convert a Python array to a numpy row vector\n",
    "x = np.array(x)\n",
    "x = x.reshape(1,np.prod(x.shape)) \n",
    "y = np.array(y)\n",
    "y = y.reshape(1,np.prod(y.shape))\n",
    "\n",
    "### numpy vectorised outer product ###\n",
    "delta_t2 = time.time()\n",
    "for k in range(1000):\n",
    "    outer2 = x.T @ y\n",
    "delta_t2 = time.time() - delta_t2\n",
    "\n",
    "print(\"{:.2f} ms vs. {:.2f} ms\".format(1000 * delta_t1, 1000 * delta_t2))\n",
    "print(\"vectorised product is {:.1f}x faster!\".format(delta_t1 / delta_t2))\n",
    "\n",
    "diff = np.sum(outer1-outer2)\n",
    "print(\"diff: \" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many linear operations such as the dot product of vectors can be expressed as matrix product. For example, if $x, y\\in \\mathbb{R}^{1\\times n}$, $x\\cdot y = x\\, y^T \\in \\mathbb{R}$ provides the dot product and $x\\otimes y=x^Ty \\in \\mathbb{R}^{n\\times n}$ gives the outer product. Study the code below carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"x.shape={}\".format(x.shape)) # (1x8) row vector\n",
    "print(\"y.shape={}\".format(x.shape)) # (1x8) row vector\n",
    "\n",
    "### vectorised dot product ###\n",
    "res = np.dot(x.flatten(),y.flatten())\n",
    "res1 = np.matmul(x,y.T).reshape(1)\n",
    "res2 = x @ y.T # preferred\n",
    "assert(res == res1)\n",
    "assert(res == res2)\n",
    "\n",
    "### vectorised outer product ###\n",
    "res = np.outer(x.flatten(),y.flatten())\n",
    "res1 = np.matmul(x.T,y)\n",
    "res2 = x.T @ y # preferred\n",
    "assert(np.sum(res-res1) == 0)\n",
    "assert(np.sum(res-res2) == 0)\n",
    "\n",
    "### vectorised element-wise multiplication ###\n",
    "res = np.multiply(x.flatten(),y.flatten())\n",
    "res1 = x * y # preferred\n",
    "assert(np.sum(res-res1) == 0)\n",
    "\n",
    "### matrix/vector product ###\n",
    "w = np.random.rand(2,x.shape[1]) # Random (2x8)\n",
    "res = np.dot(w,x.flatten())\n",
    "res1 = w @ x.T # preferred\n",
    "assert(np.sum(res-res1) == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vectorized implementation is not only more elegant but also more efficient. This is particularly important for large matrices or vectors.\n",
    "\n",
    "Use `np.multiply()` or simply `*` to perform an element-wise multiplication. Use `np.matmul()` or simply `@` to compute the product of matrices. `.T` returns the matrix transpose.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 7:**</font> Let's now implement the $L_2$ loss function using numpy vectorised operations. The $L_2$ loss is an important performance measure for models in deep learning. A large loss means that your model predictions $\\hat{y}$ are poor and far away from the ground truth $y$. Model parameters are generally optimised using gradient descent in an attempt to minimise the loss.\n",
    ">\n",
    "> Formally the $L_2$ loss is defined as $L_2(y,\\hat{y})=\\sum_{k=1}^m (\\hat{y}^k-y^k)^2$\n",
    "> \n",
    "> There are several ways to implement the $L_2$. As a reminder, if $x \\in \\mathbb{R}^{1\\times n}$, then `np.dot(x,x)` = $\\sum_{i=1}^n x_i^{2} = x\\, x^T$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def L2(y,y_hat):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    y -- vector of size m (ground truth labels)\n",
    "    y_hat -- vector of size m (predicted labels)\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    loss -- L2 loss\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (1 line)\n",
    "    loss = None\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y_hat = np.array([.8, 0.3, 0.9, .2, .1])\n",
    "y = np.array([1, 0, 1, 0, 0])\n",
    "print(\"L2 = {:.2f}\".format(L2(y,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `L2 = 0.19`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can define  the $L_1$ loss as $L_1(y,\\hat{y})=\\sum_{k=1}^m |\\hat{y}^k-y^k|$\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 8:**</font> Implement the numpy vectorized version of the $L_1$ loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def L1(y,y_hat):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    y -- vector of size m (ground truth labels)\n",
    "    y_hat -- vector of size m (predicted labels)\n",
    "\n",
    "    \n",
    "    Returns:\n",
    "    loss -- L1 loss\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (1 line)\n",
    "    loss = None\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "y_hat = np.array([.8, 0.3, 0.9, .2, .1])\n",
    "y = np.array([1, 0, 1, 0, 0])\n",
    "print(\"L1 = {:.2f}\".format(L1(y,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `L1 = 0.90`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkgray'>**Take away:**</font>\n",
    "- numpy mathematical functions are applied to `np.array` element by element.<br> numpy provides many functions such `np.dot`, `np.sum`, `np.maximum`, `np.abs`, `np.multiply`...\n",
    "- use `np.reshape` to change the dimension of tensors and check tensor shapes with `np.shape`\n",
    "- numpy broadcasting mechanism is very useful however practice plenty to use it correctly\n",
    "- vectorisation provides compact code,  and elegance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- EOF --"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
