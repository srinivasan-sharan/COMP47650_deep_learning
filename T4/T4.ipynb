{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Feedforward Neural Network #\n",
    "\n",
    "Objectives: built and train a deep layer feedforward neural network building on the single feedfoward neural network implementation of the previous session notebook.\n",
    "\n",
    "> **Instructions:** ensure your Python environment is setup with the following additional packages: \n",
    ">\n",
    "> - `t4utils.py` contains unit tests to check your code\n",
    "\n",
    "**Notation:** superscript $[k]$ refers to quantities associated with the $k$-th layer.\n",
    "\n",
    "<img src=\"figs/deepnetwork.png\" style=\"width:800px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import t4utils as t4\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of steps to implement a deep neural network model:\n",
    "\n",
    "1. Initialize the parameters of a $K$-layer neural network.\n",
    "2. Implement the forward propagation:\n",
    "    - compute linear component $z^{[k]}=a^{[k-1]} W^{[k]} + b^{[k]}$\n",
    "    - compute activation $a^{[k]} = g(z^{[k]})$, where $g(\\cdot)$ is the layer non-linearity (activation function). We'll use both ReLU and sigmoid.\n",
    "    - combine $K-1$ ReLU layers with a final sigmoid layer ($K$ layer in total)\n",
    "    - compute all layer outputs\n",
    "    - evaluate loss from final layer activation\n",
    "3. Implement the backward propagation:\n",
    "    - compute the gradients for the linear component and the non-linear activation function\n",
    "    - perform backprop on sigmoid layer followed by $K-1$ ReLU layers\n",
    "    - evaluate gradients for all parameters $W^{[k]}, b^{[k]}$\n",
    "4. Update parameters using learning rate\n",
    "\n",
    "To implement our learning algorithm, we will associate a backprop module for every forward function. At every step of your \n",
    "forward evaluation, we will store results in a cache required to compute gradients during backprop. As a result, training a network using backprop requires about twice more memory than inference. With deep learning frameworks, your generally describe your network as graph and differentiation is performed automatically for training model parameters.\n",
    "\n",
    "## A. Load Dataset and Initialise Model ##\n",
    "\n",
    "First, let's generate a dataset to work on. We will use the `catsvsnoncats` image dataset used in the logistic regression notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath='datasets/catsvsnoncats.h5'):\n",
    "    \"\"\"\n",
    "    Load and pre-process dataset\n",
    "    \n",
    "    Arguments:\n",
    "    filepath -- string, dataset path\n",
    "\n",
    "    Returns:\n",
    "    (X_train, Y_train), (X_test, Y_test), classes -- training and test datasets\n",
    "    \"\"\"\n",
    "\n",
    "    h5file = h5py.File(filepath, \"r\")\n",
    "    X_train = np.array(h5file[\"X_train\"][:])\n",
    "    Y_train = np.array(h5file[\"Y_train\"][:])\n",
    "    X_test = np.array(h5file[\"X_test\"][:])\n",
    "    Y_test = np.array(h5file[\"Y_test\"][:])\n",
    "    classes = np.array(h5file[\"Classes\"][:]) \n",
    "    h5file.close()\n",
    "\n",
    "    # Reshape and scale datasets containing N_train and N_test mages such that\n",
    "    # X_train.shape = (n_train, W * H * 3)\n",
    "    # Y_train.shape = (n_train, 1)\n",
    "    # X_test.shape = (n_test, W * H * 3)\n",
    "    # X_test.shape = (n_test, 1)\n",
    "    # Scale X_train and X_test from range {0..255} to (0,1)\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (4 lines)\n",
    "    X_train = None\n",
    "    X_test = None\n",
    "    Y_train = None\n",
    "    Y_test = None\n",
    "    ### END OF YOUR CODE SEGMENT ###\n",
    "\n",
    "    return (X_train, Y_train), (X_test, Y_test), classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now configure our model topology i.e. K-1 ReLU activated layers and one final sigmoid activated layer. Initialise bias to zeros and weights from $\\mathcal{N}(0,\\sigma)$ with $\\sigma = 1 /\\sqrt{n_h^{[k]}}$ where $n_h^{[k]}$ is number of units in the $k$-th layer.\n",
    "\n",
    "The number of units of the first layer is determined by the number of features of our dataset, $x$. Similarly the number of units of the final output layer is determined by the dimension of $y$. The number of units for the remaining $K-1$ hidden layers is specified as an array of integers.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 1:**</font> Setup the model topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config(X, Y, n_h=[128, 64, 16, 8]):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- n data samples, shape = (n, n_x)\n",
    "    Y -- ground truth label, column vector of shape (n, n_y)\n",
    "    n_h -- array with number of units in hidden layers, size K-1\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing initialised model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised bias vector of shape (1, n_h1)\n",
    "        ...\n",
    "        Wk -- initialised weight matrix of shape (n_hk-1, n_hk)\n",
    "        bk -- initialised bias vector of shape (1, n_hk)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised bias vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    n_x = None # size of input layer\n",
    "    n_y = None # size of output layer\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    dims = sum([[n_x], n_h, [n_y]], [])\n",
    "    K = len(dims) # number of network layers\n",
    "\n",
    "    params = {}\n",
    "    \n",
    "    for k in range(1, K):\n",
    "        ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "        params['W{}'.format(k)] = None\n",
    "        params['b{}'.format(k)] = None\n",
    "        ### END OF YOUR CODE SEGMENT ###  \n",
    "                                                    \n",
    "        assert(params['W{}'.format(k)].shape == (dims[k - 1], dims[k]))\n",
    "        assert(params['b{}'.format(k)].shape == (1, dims[k]))\n",
    "\n",
    "    assert(X.shape[0] == (Y.shape[0]))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m X, Y, n_h \u001b[38;5;241m=\u001b[39m t4\u001b[38;5;241m.\u001b[39mmodel_config_test()\n\u001b[0;32m----> 2\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(params) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k, params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k)]))\n",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m, in \u001b[0;36mmodel_config\u001b[0;34m(X, Y, n_h)\u001b[0m\n\u001b[1;32m     33\u001b[0m     params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m### END OF YOUR CODE SEGMENT ###  \u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m \u001b[38;5;241m==\u001b[39m (dims[k \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], dims[k]))\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k)]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m1\u001b[39m, dims[k]))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m (Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "X, Y, n_h = t4.model_config_test()\n",
    "params = model_config(X, Y, [3])\n",
    "for k in range(1, len(params) // 2 + 1):\n",
    "    print(\"W{} = {}\".format(k, params['W{}'.format(k)]))\n",
    "    print(\"b{} = {}\".format(k, params['b{}'.format(k)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    "> \n",
    "> ```\n",
    "> W1 = [[ 0.9195  -0.21968  0.58   ]\n",
    ">       [-0.28855 -0.51552 -0.41944]\n",
    ">       [ 0.15847  0.17663  0.75786]\n",
    ">       [-0.18893 -0.54634 -0.32417]]\n",
    "> b1 = [[0. 0. 0.]]\n",
    "> W2 = [[ 0.85447 -1.2155 ]\n",
    ">       [ 0.13204  0.20821]\n",
    ">       [-0.83132 -0.38261]]\n",
    "> b2 = [[0. 0.]]\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "## B. Forward Propagation ##\n",
    "\n",
    "We will now implement the forward propagation for the linear component of the $k$-th layer $z_k = a^{[k-1]} W^{[k]} + b^{[k]}$ and its activation $a^{[k]} = g(z^{[k]})$, where $g(\\cdot)$ is either the ReLU or the sigmoid function. Note that $a^{[0]} = x$. We should also return a cache for the backward propagation.\n",
    "\n",
    "For $n$ samples stacked in the rows of a input matrix $X$, the linear and activation parts are given by $Z_k = A^{[k-1]} W^{[k]} + b^{[k]}$ and $A^{[k]} = g(Z^{[k]})$\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 2:**</font> Implement forward propagation for the linearity and both activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation for linearity\n",
    "def linear_fwd(W, b, A):\n",
    "    \"\"\"\n",
    "    Linearity\n",
    "\n",
    "    Arguments:\n",
    "    W -- weight matrix, shape (n_hk-1, n_hk)\n",
    "    b -- bias row vector, shape (1, n_hk)\n",
    "    A -- input, shape (n, n_hk-1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- linear output, shape (n, n_hk)\n",
    "    cache -- dictionary for back propagation\n",
    "        W -- weight matrix\n",
    "        b -- bias row vector\n",
    "        A_prev -- input\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    Z = None\n",
    "    cache = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    assert(Z.shape == (A.shape[0], W.shape[1]))\n",
    "    return Z, cache\n",
    "\n",
    "# Forward propagation for sigmoid non-linearity\n",
    "def sigmoid_fwd(Z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(Z), same shape as Z\n",
    "    cache -- dictionary for backpropagation\n",
    "        Z -- activation's input\n",
    "    \"\"\"\n",
    "\n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    A = None\n",
    "    cache = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache\n",
    "\n",
    "# Forward propagation for ReLU non-linearity\n",
    "def relu_fwd(Z):\n",
    "    \"\"\"\n",
    "    RELU activation\n",
    "\n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of ReLU(Z), same shape as Z\n",
    "    cache -- dictionary for backpropagation\n",
    "        Z -- activation's input\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    A = None\n",
    "    cache = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m W, b, A \u001b[38;5;241m=\u001b[39m t4\u001b[38;5;241m.\u001b[39mlinear_fwd_test()\n\u001b[0;32m----> 2\u001b[0m Z, cache \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_fwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZ.T = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(Z\u001b[38;5;241m.\u001b[39mT))\n\u001b[1;32m      4\u001b[0m A, cache \u001b[38;5;241m=\u001b[39m relu_fwd(Z)\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mlinear_fwd\u001b[0;34m(W, b, A)\u001b[0m\n\u001b[1;32m     21\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m### END OF YOUR CODE SEGMENT ###  \u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(\u001b[43mZ\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m \u001b[38;5;241m==\u001b[39m (A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], W\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Z, cache\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "W, b, A = t4.linear_fwd_test()\n",
    "Z, cache = linear_fwd(W, b, A)\n",
    "print(\"Z.T = {}\".format(Z.T))\n",
    "A, cache = relu_fwd(Z)\n",
    "print(\"A.T = ReLU(Z.T) = {}\".format(A.T))\n",
    "A, cache = sigmoid_fwd(Z)\n",
    "print(\"A.T = sigmoid(Z.T) = {}\".format(A.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    "> \n",
    "> ```\n",
    "> Z.T = [[0.71591 1.45199]]\n",
    "> A.T = ReLU(Z.T) = [[0.71591 1.45199]]\n",
    "> A.T = sigmoid(Z.T) = [[0.67171 0.8103 ]]\n",
    "> ```\n",
    "\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 3:**</font> Implement the single layer forward propagation<br>\n",
    "> Combine the forward modules to compute a single layer forward propagation $A^{[k]} = g(Z^{[k]}) = g(A^{[k-1]} W^{[k]})$, where $g(\\cdot)$ is the non-linearity (either ReLU or Sigmoid). Also return the cache for the layer's backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer forward propagation for \n",
    "def singlelayer_fwd(W, b, A_prev, non_linearity='ReLU'):\n",
    "    \"\"\"\n",
    "    Single layer forward propagation (linear + non-linearity)\n",
    "\n",
    "    Arguments:\n",
    "    W -- weight matrix, shape (n_hk-1, n_hk)\n",
    "    b -- bias row vector, shape (1, n_hk)\n",
    "    A_prev -- input, shape (n, n_hk-1)\n",
    "    non_linearity -- string ('ReLU' or 'Sigmoid') activation for layer\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of g(A_prev @ W + b), shape (n, n_hk)\n",
    "    cache -- dictionary for backprop\n",
    "        LINEAR -- dictionary linear cache\n",
    "        ACTIVATION -- dictionary activation cache        \n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (1 line)\n",
    "    Z, linear_cache = None\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    if non_linearity == 'ReLU':\n",
    "        ### INPUT YOUR CODE HERE ### (1 line)\n",
    "        A, activation_cache = None\n",
    "        ### END OF YOUR CODE SEGMENT ### \n",
    "    elif non_linearity == 'Sigmoid':\n",
    "        ### INPUT YOUR CODE HERE ### (1 line)\n",
    "        A, activation_cache = None\n",
    "        ### END OF YOUR CODE SEGMENT ###\n",
    "    \n",
    "    assert(A.shape == (A_prev.shape[0], W.shape[1]))\n",
    "    return A, {'LINEAR': linear_cache, 'ACTIVATION': activation_cache}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m W, b, A_prev \u001b[38;5;241m=\u001b[39m t4\u001b[38;5;241m.\u001b[39msinglelayer_fwd_test()\n\u001b[0;32m----> 2\u001b[0m A, cache \u001b[38;5;241m=\u001b[39m \u001b[43msinglelayer_fwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSigmoid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA_Sigmoid.T = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(A\u001b[38;5;241m.\u001b[39mT))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(cache))\n",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m, in \u001b[0;36msinglelayer_fwd\u001b[0;34m(W, b, A_prev, non_linearity)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mSingle layer forward propagation (linear + non-linearity)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    ACTIVATION -- dictionary activation cache        \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m### INPUT YOUR CODE HERE ### (1 line)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m Z, linear_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m### END OF YOUR CODE SEGMENT ### \u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m non_linearity \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReLU\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m### INPUT YOUR CODE HERE ### (1 line)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "W, b, A_prev = t4.singlelayer_fwd_test()\n",
    "A, cache = singlelayer_fwd(W, b, A_prev, 'Sigmoid')\n",
    "print(\"A_Sigmoid.T = {}\".format(A.T))\n",
    "print(\"cache = {}\".format(cache))\n",
    "A, cache = singlelayer_fwd(W, b, A_prev, 'ReLU')\n",
    "print(\"A_ReLU.T = {}\".format(A.T))\n",
    "print(\"cache = {}\".format(cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> A_Sigmoid.T = [[0.67171 0.8103 ]]\n",
    "> cache = {'LINEAR': {'W': array([[ 0.57376],\n",
    ">                                 [ 0.28773],\n",
    ">                                 [-0.23563]]), \n",
    ">                     'b': array([[0.95349]]), \n",
    ">                     'A_prev': array([[-0.21768,  0.82146,  1.48128],\n",
    ">                                      [ 1.33186, -0.36187,  0.68561]])}, \n",
    ">                     'ACTIVATION': {'Z': array([[0.71591],\n",
    ">                                                [1.45199]])}}\n",
    "> A_ReLU.T = [[0.71591 1.45199]]\n",
    "> cache = {'LINEAR': {'W': array([[ 0.57376],\n",
    ">                                 [ 0.28773],\n",
    ">                                 [-0.23563]]), \n",
    ">                     'b': array([[0.95349]]), \n",
    ">                     'A_prev': array([[-0.21768,  0.82146,  1.48128],\n",
    ">                                      [ 1.33186, -0.36187,  0.68561]])},\n",
    ">                     'ACTIVATION': {'Z': array([[0.71591],\n",
    ">                                               [1.45199]])}}\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "Create a $K$-layer deep model by stacking single layer forward propagation. The first $K-1$ layers use the ReLU non-linearity for their activation function and the sigmoid function is applied on the final layer.\n",
    "\n",
    "The loss for $n_{train}$ stacked examples is given by:\n",
    "$$\\mathcal{L}(Y, A^{[K]}) = -\\frac{1}{n_{train}}\\left(\n",
    "Y^T\\, \\log A^{[K]} + (1-Y)^T\\,\\log(1-A^{[K]})\n",
    "\\right)$$\n",
    "\n",
    "where $A^{[K]}$ is the final layer activation.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 4:**</font> Implement the $K$-layer forward propagation and compute the loss<br>\n",
    ">\n",
    "> **Note** that it is not possible to vectorise the network depth and a loop must be used. This is the only loop in our model. Keep a record of the caches for all $K$ layers in an python list (use `list.append(element)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation (inference)\n",
    "def forward_prop(params, X, Y=None):\n",
    "    \"\"\"\n",
    "    Compute the layer activations and loss if needed\n",
    "\n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    X -- n data samples, shape = (n, n_x)\n",
    "    Y -- optional argument, ground truth label, column vector of shape (n, n_y)\n",
    "\n",
    "    Returns:\n",
    "    A -- final layer output (activation value) \n",
    "    loss -- cross-entropy loss or NaN if Y=None\n",
    "    caches -- array of caches for the K layers\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    K = len(params) >> 1\n",
    "    A = X\n",
    "    \n",
    "    # K-1 [Linear->ReLU] layer\n",
    "    for k in range(1, K):\n",
    "        A_prev = A\n",
    "        ### INPUT YOUR CODE HERE ### (3 lines)\n",
    "        W = None\n",
    "        b = None\n",
    "        A, cache = None\n",
    "        ### END OF YOUR CODE SEGMENT ### \n",
    "        caches.append(cache)\n",
    "\n",
    "    # 1 [Linear->Sigmoid] layer\n",
    "    A_prev = A\n",
    "    ### INPUT YOUR CODE HERE ### (3 lines)\n",
    "    W = None\n",
    "    b = None\n",
    "    A, cache = None\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    caches.append(cache)\n",
    "    \n",
    "    loss = float('nan')\n",
    "    if Y is not None:\n",
    "        Y_hat = A\n",
    "        n = Y.shape[0]\n",
    "        # Compute the cross-entropy loss\n",
    "        ### INPUT YOUR CODE HERE ### (1 line)\n",
    "        loss = None\n",
    "        ### END OF YOUR CODE SEGMENT ###  \n",
    "\n",
    "        loss = np.squeeze(loss)\n",
    "        assert(loss.dtype == float)\n",
    "        \n",
    "    assert(A.shape == (X.shape[0], W.shape[1]))\n",
    "    return A, loss, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, params = t4.forward_prop_test()\n",
    "A, loss, caches = forward_prop(params, X, Y)\n",
    "print(\"A.T = {}\".format(A.T))\n",
    "print(\"loss = {:.5f}\".format(loss))\n",
    "print(\"{} caches\".format(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***<br>\n",
    ">\n",
    "> ```\n",
    "> A.T = [[0.10101 0.33321 0.01484 0.00042]]\n",
    "> loss = 0.67831\n",
    "> 3 caches\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "## C. Backward Propagation ##\n",
    "\n",
    "Backward propagation is implemented similarly to the forward propagation, module by module using the forward prop cache to compute gradients of the loss with respect to the model parameters $\\theta \\equiv (W^{[1]}, b^{[1]}, ..., W^{[K]}, b^{[K]})$.\n",
    "\n",
    "<img src=\"figs/backprop.png\" style=\"width:600px;\">\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 5:**</font> Implement backward propagation for the linearity and both activation functions. <br>\n",
    ">\n",
    "> Use the following gradient expressions for the linear backprop:\n",
    "> \n",
    "> $$dW^{[k]} \\equiv \\frac{\\partial \\mathcal{L} }{\\partial W^{[k]}} = \\frac{1}{n}  A^{[k-1]T}\\,dZ^{[k]} \\notag$$\n",
    ">\n",
    "> $$db^{[k]}_j\\equiv \\frac{\\partial \\mathcal{L} }{\\partial b_j^{[k]}} =\\frac{1}{n} \\sum_{i=1}^{n} dZ^{[2]}_{i\\,j} \\notag$$\n",
    ">\n",
    "> $$dA^{[k-1]} \\equiv \\frac{\\partial \\mathcal{L} }{\\partial A^{[k-1]}} = dZ^{[k]} W^{[k] T}  \\notag$$\n",
    ">\n",
    "> if $g(\\cdot)$ denotes the activation function (ReLU or Sigmoid), the gradient is as follows:\n",
    "> \n",
    "> $$dZ^{[k]} = dA^{[k]} \\odot g'(Z^{[k]}) \\notag$$.  \n",
    "> \n",
    "> Applying the above expression to the ReLU function yields the following backprop:\n",
    ">\n",
    "> $$dZ^{[k]} \\equiv \\frac{\\partial \\mathcal{L} }{\\partial Z^{[k]}} = dA^{[k]}\\odot\\mathbb 1\\text{{$Z^{[k]}$ $\\geqslant 0$}}\\notag$$\n",
    ">\n",
    "> For the Sigmoid backprop, the backprop expression is:\n",
    ">\n",
    "> $$dZ^{[k]} \\equiv \\frac{\\partial \\mathcal{L} }{\\partial Z^{[k]}} = dA^{[k]}\\odot\\sigma(Z^{[k]})\\odot\\left(1-\\sigma(Z^{[k]})\\right)\\notag$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation for linearity\n",
    "def linear_back(dZ, cache):\n",
    "    \"\"\"\n",
    "    Linearity backprop\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- gradient of loss with respect to current layer linear output\n",
    "    cache -- dictionary from forward propagation\n",
    "        W -- weight matrix\n",
    "        b -- bias row vector\n",
    "        A_prev -- previous layer activation input\n",
    "\n",
    "    Returns:\n",
    "    dW -- gradient of loss with respect to current layer weights\n",
    "    db -- gradient of loss with respect to current layer bias\n",
    "    dA_prev -- gradient of loss with respect to activation of previous layer output\n",
    "    \"\"\"\n",
    "\n",
    "    ### INPUT YOUR CODE HERE ### (7 lines)\n",
    "    W = None\n",
    "    b = None\n",
    "    A_prev = None\n",
    "    n = None\n",
    "    dW = None\n",
    "    db = None\n",
    "    dA_prev = None\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dW, db, dA_prev\n",
    "\n",
    "# Backward propagation for ReLU non-linearity\n",
    "def relu_back(dA, cache):\n",
    "    \"\"\"\n",
    "    ReLU backprop\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of loss with respect to activation\n",
    "    cache -- dictionary from forward propagation\n",
    "        Z -- layer linearity output \n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of loss with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (3 lines)\n",
    "    Z = None\n",
    "    dZ = None\n",
    "    None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "\n",
    "# Backward propagation for sigmoid non-linearity\n",
    "def sigmoid_back(dA, cache):\n",
    "    \"\"\"\n",
    "    Sigmoid backprop\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of loss with respect to activation\n",
    "    cache -- dictionary from forward propagation\n",
    "        Z -- layer linearity output \n",
    "\n",
    "    Returns:\n",
    "    dZ -- gradient of loss with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (3 lines)\n",
    "    Z = None\n",
    "    S = None\n",
    "    dZ = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ, cache = t4.linear_back_test()\n",
    "dW, db, dA_prev = linear_back(dZ, cache)\n",
    "print(\"dW.T = {}\".format(dW.T))\n",
    "print(\"db = {}\".format(db))\n",
    "print(\"dA_prev = {}\".format(dA_prev))\n",
    "A, cache = t4.non_linearity_test()\n",
    "dZ = relu_back(A, cache)\n",
    "print(\"ReLU: dZ = {}\".format(dZ))\n",
    "dZ = sigmoid_back(A, cache)\n",
    "print(\"Sigmoid: dZ = {}\".format(dZ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> dW.T = [[0.12038 0.0907  0.15756]]\n",
    "> db = [[0.30189]]\n",
    "> dA_prev = [[ 0.05129 -0.20755  0.3678 ]\n",
    ">            [-0.19356  0.78325 -1.38795]]\n",
    "> ReLU: dZ = [[-0.21768  0.82146  0.     ]\n",
    ">             [ 1.33186  0.       0.     ]]\n",
    "> Sigmoid: dZ = [[-0.05018  0.20117  0.36523]\n",
    ">                [ 0.26743 -0.0476   0.1664 ]]\n",
    "> ```\n",
    "\n",
    "*** \n",
    "\n",
    "We can now combine the backprop from the linear and non-lineary modules to compute the backprop for a single activation layer.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 6:**</font> Implement the single layer backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singlelayer_back(dA, cache, non_linearity='ReLU'):\n",
    "    \"\"\"\n",
    "    Single layer backprop (linear + non-linearity)\n",
    "\n",
    "    Arguments:\n",
    "    dA -- gradient of loss with respect to activation\n",
    "    cache -- dictionary from forward propagation\n",
    "        LINEAR -- dictionary from forward linear propagation \n",
    "        ACTIVATION -- dictionary from forward non-linearity propagation \n",
    "    non_linearity -- string ('ReLU' or 'Sigmoid') activation for layer\n",
    "\n",
    "    Returns:\n",
    "    dW -- gradient of loss with respect to current layer weights\n",
    "    db -- gradient of loss with respect to current layer bias\n",
    "    dA_prev -- gradient of loss with respect to activation of previous layer output\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache = cache['LINEAR']\n",
    "    activation_cache = cache['ACTIVATION']\n",
    "    \n",
    "    if non_linearity == 'Sigmoid':\n",
    "        ### INPUT YOUR CODE HERE ### (1 line)\n",
    "        dZ = None\n",
    "        ### END OF YOUR CODE SEGMENT ###  \n",
    "    elif non_linearity == 'ReLU':\n",
    "        ### INPUT YOUR CODE HERE ### (1 line)\n",
    "        dZ = None\n",
    "        ### END OF YOUR CODE SEGMENT ###  \n",
    "        \n",
    "    ### INPUT YOUR CODE HERE ### (1 line)\n",
    "    dW, db, dA_prev = None\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA, cache = t4.singlelayer_back_test()\n",
    "\n",
    "dW, db, dA_prev = singlelayer_back(dA, cache, 'ReLU')\n",
    "print(\"ReLU: dW.T = {}\".format(dW.T))\n",
    "print(\"ReLU: db = {}\".format(db))\n",
    "print(\"ReLU: dA_prev = {}\".format(dA_prev))\n",
    "\n",
    "dW, db, dA_prev = singlelayer_back(dA, cache, 'Sigmoid')\n",
    "print(\"Sigmoid: dW.T = {}\".format(dW.T))\n",
    "print(\"Sigmoid: db = {}\".format(db))\n",
    "print(\"Sigmoid: dA_prev = {}\".format(dA_prev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***<br>\n",
    ">\n",
    "> ```\n",
    "> ReLU: dW.T = [[-0.16122 -0.14496  0.03939]]\n",
    "> ReLU: db = [[-0.10884]]\n",
    "> ReLU: dA_prev = [[ 0.05129 -0.20755  0.3678 ]\n",
    ">                  [ 0.       0.       0.     ]]\n",
    "> Sigmoid: dW.T = [[0.02563 0.01894 0.03751]]\n",
    "> Sigmoid: db = [[0.06896]]\n",
    "> Sigmoid: dA_prev = [[ 0.01282 -0.05188  0.09194]\n",
    ">                     [-0.04532  0.18338 -0.32496]]\n",
    "> ```\n",
    "\n",
    "*** \n",
    "\n",
    "Now that we have the backprop implemented for a single layer, we can compute the backprop for a $K$-layer network iteratively. At each iteration, we use the layer cache computed during forward propagation, initialising the back-propagation with $dA^{[K]}\\equiv \\frac{\\partial \\mathcal{L}}{\\partial A^{[K]}}$, given for sample $i$ as\n",
    "\n",
    "$$ dA_i^{[K]} \\equiv \\frac{\\partial \\mathcal{L}}{\\partial A_i^{[K]}} = - \\left( \\frac{Y_i}{A^{[K]}} - \\frac{1- Y_i}{1-A^{[K]}} \\right)$$\n",
    "\n",
    "Using the post-activation gradient $dA^{[K]}$, initialise the backprop chain and feed the backprop function for the last network layer (sigmoid). Going backward in the network with `for` loop, continue the backprop evaluation for the remaining $K-1$ layers (ReLU activation). Store each of the $dW^{[k]}, db^{[k]}, dA^{[k]}$ gradients, these will be required to update our model parameters.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 7:**</font> Implement the model backprop<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward_propagation\n",
    "def back_prop(AK, Y, caches):\n",
    "    \"\"\"\n",
    "    Compute back-propagation gradients\n",
    "    \n",
    "    Arguments:\n",
    "    AK -- probability vector, final layer output, shape (1, n_y)\n",
    "    Y -- ground truth output (n, n_y)\n",
    "    caches -- array of layer cache, len=K\n",
    "    \n",
    "    Returns:\n",
    "    grads -- dictionary containing your gradients with respect to all parameters\n",
    "        dW1 -- weight gradient matrix of shape (n_x, n_h1)\n",
    "        db1 -- bias gradient vector of shape (1, n_h1)\n",
    "        ...\n",
    "        dWK -- weight gradient matrix of shape (n_hK-1, n_y)\n",
    "        dbK -- bias gradient vector of shape (1, n_y)\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    K = len(caches)\n",
    "    n = AK.shape[0]\n",
    "    assert(Y.shape == AK.shape)\n",
    "\n",
    "    ### INPUT YOUR CODE HERE ### (3 lines)\n",
    "    dAK = None\n",
    "    cache = None \n",
    "    grads[\"dW{}\".format(K)], grads[\"db{}\".format(K)],  grads[\"dA{}\".format(K)] = None\n",
    "    ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    for k in reversed(range(K - 1)):\n",
    "        ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "        cache = None\n",
    "        grads[\"dW{}\".format(k + 1)], grads[\"db{}\".format(k + 1)], grads[\"dA{}\".format(k + 1)] = None\n",
    "        ### END OF YOUR CODE SEGMENT ### \n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AK, Y, caches = t4.back_prop_test()\n",
    "grads = back_prop(AK, Y, caches)\n",
    "print(\"dW1.T = {}\".format(grads['dW1'].T))\n",
    "print(\"db1 = {}\".format(grads['db1']))\n",
    "print(\"dA1 = {}\".format(grads['dA1']))\n",
    "print(\"dW2.T = {}\".format(grads['dW2'].T))\n",
    "print(\"db2 = {}\".format(grads['db2']))\n",
    "print(\"dA2 = {}\".format(grads['dA2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ``` \n",
    "> dW1.T = [[ 0.39632  0.35635 -0.09682  0.18344]\n",
    ">          [ 0.71849  0.64602 -0.17552  0.33255]\n",
    ">          [ 0.16057  0.14437 -0.03923  0.07432]]\n",
    "> db1 = [[0.26756 0.48505 0.1084 ]]\n",
    "> dA1 = [[-1.2351  -0.07942  2.06421 -1.18989]\n",
    ">        [ 0.       0.       0.       0.     ]]\n",
    "> dW2.T = [[-0.68133  0.1135  -0.30305]]\n",
    "> db2 = [[0.86059]]\n",
    "> dA2 = [[0.53511 0.9701  0.2168 ]\n",
    ">        [0.47488 0.86091 0.1924 ]]\n",
    "> ```\n",
    "\n",
    "*** \n",
    "\n",
    "The update rule for gradient descent is given by $\\theta = \\theta - \\lambda\\, d\\theta$, where $\\lambda$ denotes the learning rate. The choice for the value of hyper-parameter $\\lambda$ is important, a small value will lead to poor convergence while a value too large may result in divergence.\n",
    "\n",
    "In our deep network, model parameters $\\theta$ decribe the tuple\n",
    "$(W^{[1]}, b^{[1]}, \\ldots, W^{[K]}, b^{[K]})$.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 8:**</font> Implement the update rule in function `update_params()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model parameters\n",
    "def update_params(params, grads, learning_rate=0.8):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    grads -- dictionary containing gradients\n",
    "        dW1 -- weight gradient matrix of shape (n_x, n_h1)\n",
    "        db1 -- bias gradient vector of shape (1, n_h1)\n",
    "        ...\n",
    "        dWK -- weight gradient matrix of shape (n_hK-1, n_y)\n",
    "        dbK -- bias gradient vector of shape (1, n_y)\n",
    "    learning_rate -- learning rate of the gradient descent (hyperparameter)\n",
    "\n",
    "    Returns:\n",
    "    params -- dictionary containing updated parameters\n",
    "    \"\"\"\n",
    "\n",
    "    K = len(params) >> 1\n",
    "    for k in range(1, K + 1):\n",
    "        ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "        params['W{}'.format(k)] = None\n",
    "        params['b{}'.format(k)] = None\n",
    "        ### END OF YOUR CODE SEGMENT ### \n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, grads = t4.update_params_test()\n",
    "params = update_params(params, grads, 0.1)\n",
    "print(\"W1.T = {}\".format(params['W1'].T))\n",
    "print(\"b1 = {}\".format(params['b1']))\n",
    "print(\"W2.T = {}\".format(params['W2'].T))\n",
    "print(\"b2 = {}\".format(params['b2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> W1.T = [[-0.39654  1.51821  0.58204  1.00121]\n",
    ">         [ 0.7778  -0.33413  0.35043 -1.55824]\n",
    ">         [ 1.47163  0.72108 -0.23125 -0.4334 ]]\n",
    "> b1 = [[-0.07123 -0.68594  0.23951]]\n",
    "> W2.T = [[-0.14884  2.72671  0.61945]]\n",
    "> b2 = [[0.74769]]\n",
    "> ```\n",
    "\n",
    "*** \n",
    "\n",
    "You can implement the training algorithm using gradient descent with backprop.\n",
    "\n",
    "> <font color='darkgreen'>**Exercise 9:**</font> Complete the function below to train your deep neural network model by repeatedly performing inference (forward propagation), evaluating parameter gradients using backprop, and updating parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter optimisation using backprop\n",
    "def model_fit(params, X, Y, epochs=2500, learning_rate=0.0075, verbose=False):\n",
    "    \"\"\"\n",
    "    Optimise model parameters by performing gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    X -- n data samples  (n, n_x)\n",
    "    Y -- groud truth label vector of size (n, n_y)\n",
    "    epochs -- number of iteration updates through dataset\n",
    "    learning_rate -- learning rate of the gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary with optimised parameters\n",
    "    grads -- dictionary with final gradients\n",
    "    loss_log -- list of loss values for every 100 updates\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_log = []\n",
    "    for i in range(epochs):\n",
    "        ### INPUT YOUR CODE HERE ### (3 lines)\n",
    "        A, loss, caches = None\n",
    "        grads = None\n",
    "        params = None\n",
    "        ### END OF YOUR CODE SEGMENT ###  \n",
    "        \n",
    "        # logs\n",
    "        if i % 100 == 0:\n",
    "            loss_log.append(loss.item())\n",
    "            if verbose:\n",
    "                print(\"Loss after {} epochs: {:.3f}\".format(i, loss))\n",
    "     \n",
    "    return params, grads, loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, params = t4.forward_prop_test()\n",
    "params, grads, loss_log = model_fit(params, X, Y, epochs = 300, verbose=False)\n",
    "print(\"loss = {}\".format(np.array(loss_log)))\n",
    "print(\"W1.T = {}\".format(params['W1'].T))\n",
    "print(\"b1 = {}\".format(params['b1']))\n",
    "print(\"W2.T = {}\".format(params['W2'].T))\n",
    "print(\"b2 = {}\".format(params['b2']))\n",
    "print(\"W3.T = {}\".format(params['W3'].T))\n",
    "print(\"b3 = {}\".format(params['b3']))\n",
    "print(\"dW1.T = {}\".format(grads['dW1'].T))\n",
    "print(\"db1 = {}\".format(grads['db1']))\n",
    "print(\"dW2.T = {}\".format(grads['dW2'].T))\n",
    "print(\"db2 = {}\".format(grads['db2']))\n",
    "print(\"dW3.T = {}\".format(grads['dW3'].T))\n",
    "print(\"db3 = {}\".format(grads['db3']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    "> \n",
    "> ```\n",
    "> loss = [0.67831 0.39311 0.38196]\n",
    "> W1.T = [[-0.93729  0.72484 -0.71946  0.90889 -0.95217]\n",
    ">         [ 0.06497  0.47414 -0.83891  0.21742  0.58681]\n",
    ">         [-3.22537  0.40678 -0.81217 -1.18205  1.06676]\n",
    ">         [ 1.02805  0.46416 -0.46209 -0.91687  0.2003 ]]\n",
    "> b1 = [[-0.89367 -0.9791  -1.53733 -1.24609]]\n",
    "> W2.T = [[ 0.33411 -0.79252  1.2068  -0.83628]\n",
    ">         [-0.17309  1.839   -0.57709  0.31693]\n",
    ">         [ 1.46725 -0.43936 -1.03104  0.35325]]\n",
    "> b2 = [[ 1.44688 -0.37785 -1.12293]]\n",
    "> W3.T = [[-0.48626  1.47999 -2.10593]]\n",
    "> b3 = [[0.35461]]\n",
    "> dW1.T = [[-0.00422  0.00449  0.03353  0.02765 -0.00591]\n",
    ">          [ 0.       0.       0.       0.       0.     ]\n",
    ">          [ 0.01105  0.00226 -0.00011  0.00337 -0.0016 ]\n",
    ">          [ 0.       0.       0.       0.       0.     ]]\n",
    "> db1 = [[ 0.01863  0.      -0.00654  0.     ]]\n",
    "> dW2.T = [[ 0.02712  0.      -0.02508  0.     ]\n",
    ">        [ 0.       0.       0.       0.     ]\n",
    ">        [-0.00808  0.       0.       0.     ]]\n",
    "> db2 = [[ 0.01661  0.      -0.00378]]\n",
    "> dW3.T = [[-0.00584  0.       0.00361]]\n",
    "> db3 = [[-0.03416]]\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "Once the model parameters are optimised, we can use the deep feedforward neural network model to predict the class of a given input $x$. \n",
    "\n",
    "> <font color='darkgreen'>**Exercise 10:**</font> Implement the `model_predict()` function. <br/> <br/>\n",
    "> Perform model inference on input $X$ and convert the activation output to predictions (0/1 values): <br/>\n",
    ">\n",
    "> $\\hat{Y_i}={\\mathbb 1}_{A_i^{[K]}>1/2} = \n",
    "    \\begin{cases}\n",
    "      1 & \\text{if } A_i^{[K]}> 1/2 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  \n",
    ">    \n",
    "> Avoid python iteration and `if` statements preferring numpy vectorial code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model inference\n",
    "def model_predict(params, X):\n",
    "    \"\"\"\n",
    "    Predict class label using model parameters\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing model parameters\n",
    "        W1 -- initialised weight matrix of shape (n_x, n_h1)\n",
    "        b1 -- initialised weight matrix of shape (1, n_h1)\n",
    "        ...\n",
    "        WK -- initialised weight matrix of shape (n_hK-1, n_y)\n",
    "        bK -- initialised weight matrix of shape (1, n_y)\n",
    "    X -- n data samples, shape (n, n_x)\n",
    "    \n",
    "    Returns:\n",
    "    Y_hat -- vector with class predictions for examples in X, shape (n, n_y)\n",
    "    \"\"\"\n",
    "        \n",
    "    ### INPUT YOUR CODE HERE ### (2 lines)\n",
    "    AK, _, _ = None\n",
    "    Y_hat = None # Convert activations to {0,1} predictions\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "        \n",
    "    n = X.shape[0]\n",
    "    assert(Y_hat.shape == (n, 1))    \n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _, params = t4.forward_prop_test()\n",
    "params['b1'] = np.zeros((1,4))\n",
    "Y_hat = model_predict(params, X)\n",
    "print(\"predictions.T = {}\".format(Y_hat.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> `predictions.T = [[0. 1. 0. 0.]]`\n",
    "\n",
    "***\n",
    "\n",
    "## D. Stitching it all together ##\n",
    "\n",
    "We can now combine all the previous step into a model function that will take our training and testing datasets as input along with our hyper-parameters (learning rate, number of epochs for training and number of units in hidden the layers). It will return trained model parameters along with training and testing logs useful to study the training behaviour of our backprop optimisation. \n",
    "\n",
    "> <font color='darkgreen'>**Exercise 11:**</font> Implement the following function and evaluate your model on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep model\n",
    "def deep_model(X_train, Y_train, X_test, Y_test, hidden_layers=[21, 7, 3], epochs=2500, learning_rate=0.007, verbose=True):\n",
    "    '''\n",
    "    Build, train and evalaute the K-layer model\n",
    "    (K-1) * [LINEAR -> RELU]  -> [LINEAR -> SIGMOID] \n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set a numpy array of shape (n_train, n_x)\n",
    "    Y_train -- training groud truth vector of size (n_train, n_y)\n",
    "    X_test -- testing set a numpy array of shape (n_test, n_x)\n",
    "    Y_test -- testing groud truth vector of size (n_test, n_y)\n",
    "    hidden_layers -- array with number of units in hidden layers\n",
    "    epochs -- number of iteration updates through dataset for training (hyperparameter)\n",
    "    learning_rate -- learning rate of the gradient descent (hyperparameter)\n",
    "    \n",
    "    Returns:\n",
    "    model -- dictionary \n",
    "        PARAMS -- parameters\n",
    "        LOSS -- log of training loss\n",
    "        GRADS -- final \n",
    "        ACC -- array with training and testing accuracies\n",
    "        LR -- learning rate\n",
    "    '''\n",
    "    \n",
    "    ### INPUT YOUR CODE HERE ### (6 lines)\n",
    "    params = None\n",
    "    params, grads, loss = None\n",
    "    Y_hat_train = None\n",
    "    Y_hat_test = None\n",
    "    train_acc = None\n",
    "    test_acc = None\n",
    "    ### END OF YOUR CODE SEGMENT ###  \n",
    "\n",
    "    print(\"{:.1f}% training acc.\".format(train_acc))\n",
    "    print(\"{:.1f}% test acc.\".format(test_acc))\n",
    "        \n",
    "    return {\"PARAMS\": params, \"LOSS\": loss, \"GRADS\": grads, \"ACC\": [train_acc, test_acc], \"LR\": learning_rate}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Evaluate the following cell to train a single hidden layer network with 7 units on the `catsvsnoncats` dataset. Loss should be decreasing and it will take a few minutes to complete the 2500 training epochs. Interrupt the evaluation if the loss after 200 epochs differs from expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test), classes = load_dataset('datasets/catsvsnoncats.h5')\n",
    "np.random.seed(2019)\n",
    "model = deep_model(X_train, Y_train, X_test, Y_test, hidden_layers=[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    "> ```\n",
    "> Loss after 0 epochs: 0.747\n",
    "> Loss after 100 epochs: 0.529\n",
    "> Loss after 200 epochs: 0.469\n",
    "> ...\n",
    "> Loss after 2300 epochs: 0.026\n",
    "> Loss after 2400 epochs: 0.024\n",
    "> 100.0% training acc.\n",
    "> 70.0% test acc.\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "The performance is similar to what was achieved with the logistic regression model. Note that training accuracy is 100% while testing is around 70%. This is a sign of overfitting.\n",
    "\n",
    "***\n",
    "\n",
    "Let's now train a 5-layer model on the same dataset. Evaluate the following cell. It will take a few minutes to complete the 2500 epoch of training. Interrupt the evaluation if the loss after 200 epochs differs from expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test), classes = load_dataset('datasets/catsvsnoncats.h5')\n",
    "np.random.seed(2019)\n",
    "model = deep_model(X_train, Y_train, X_test, Y_test, hidden_layers=[21, 9, 7], learning_rate=0.009)\n",
    "\n",
    "plt.plot(model[\"LOSS\"])\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs (x100)')\n",
    "plt.title(\"Learning rate = {}\".format(model[\"LR\"]))\n",
    "plt.show()\n",
    "\n",
    "params = model[\"PARAMS\"]\n",
    "parameter_count = 0\n",
    "for key in params.keys():\n",
    "    parameter_count = parameter_count + np.prod(params[key].shape)\n",
    "print(\"Number of trainable parameters: {}\".format(parameter_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Expected Output:***\n",
    ">\n",
    ">```\n",
    "> Loss after 0 epochs: 0.772\n",
    "> ...\n",
    "> Loss after 2200 epochs: 0.028\n",
    "> Loss after 2300 epochs: 0.025\n",
    "> Loss after 2400 epochs: 0.023\n",
    "> 100.0% training acc.\n",
    "> 78.0% test acc.\n",
    "> \n",
    "> Number of trainable parameters: 258345\n",
    "> ```\n",
    "\n",
    "***\n",
    "\n",
    "This deeper network has a little over 1/4 million parameters for a dataset of $64\\times 64$ colour images. Although we did outperform our previous models, this is far from efficient. Also our model suffers from overfitting. Tuning hyper-parameters such number of hidden layers, number of units, learning rate or number of epochs also greatly affect performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- EOF --"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
